{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 09:29:38.352086: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-21 09:29:38.526164: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-21 09:29:38.532867: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-21 09:29:38.532883: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-21 09:29:38.564864: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-21 09:29:39.682659: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-21 09:29:39.682727: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-21 09:29:39.682734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#Import Modules\n",
    "import uproot\n",
    "import awkward\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.figure as figure\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from Sum import Sum\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from numpy.lib.recfunctions import structured_to_unstructured\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "MASKVAL = -999\n",
    "MAXTRACKS = 8\n",
    "BATCHSIZE = 64\n",
    "EPOCHS = 1000\n",
    "MAXEVENTS = 99999999999999999\n",
    "# VALFACTOR = 10\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "\n",
    "# Define Early Stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001,\n",
    "    patience = 20,\n",
    "    restore_best_weights = True,\n",
    ")\n",
    "\n",
    "#Define ReducedLR\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8,\n",
    "                              patience=10, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the root file\n",
    "tree = uproot.open(\"hffrag.root:CharmAnalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which branches of the tree we actually want to look at\n",
    "# Not currently used!\n",
    "branches = \\\n",
    "  [ \\\n",
    "\n",
    "  # true jet information\n",
    "   \"AnalysisAntiKt4TruthJets_pt\"\n",
    "   , \"AnalysisAntiKt4TruthJets_eta\"\n",
    "   , \"AnalysisAntiKt4TruthJets_phi\"\n",
    "   , \"AnalysisAntiKt4TruthJets_m\"\n",
    "\n",
    "\n",
    "  # true b-hadron information\n",
    "  # these b-hadrons are inside the truth jets\n",
    "   , \"AnalysisAntiKt4TruthJets_ghostB_pdgId\"\n",
    "    , \"AnalysisAntiKt4TruthJets_ghostB_pt\"\n",
    "   , \"AnalysisAntiKt4TruthJets_ghostB_eta\"\n",
    "   , \"AnalysisAntiKt4TruthJets_ghostB_phi\"\n",
    "   , \"AnalysisAntiKt4TruthJets_ghostB_m\"\n",
    "  \n",
    "\n",
    "  # reconstructed jet information\n",
    "   , \"AnalysisJets_pt_NOSYS\"\n",
    "   , \"AnalysisJets_eta\"\n",
    "   , \"AnalysisJets_phi\"\n",
    "   , \"AnalysisJets_m\"\n",
    "\n",
    "\n",
    "  # reconstructed track information\n",
    "  , \"AnalysisTracks_pt\"\n",
    "  , \"AnalysisTracks_eta\"\n",
    "  , \"AnalysisTracks_phi\"\n",
    "  , \"AnalysisTracks_z0sinTheta\"\n",
    "  , \"AnalysisTracks_d0sig\"\n",
    "  , \"AnalysisTracks_d0\"\n",
    "  , \"AnalysisTracks_d0sigPV\"\n",
    "  , \"AnalysisTracks_d0PV\"\n",
    "  ]\n",
    "\n",
    "\n",
    "  # True jet information\n",
    "jetfeatures = \\\n",
    "  [ \"AnalysisAntiKt4TruthJets_pt\"\n",
    "  , \"AnalysisAntiKt4TruthJets_eta\"\n",
    "  , \"AnalysisAntiKt4TruthJets_phi\"\n",
    "  , \"AnalysisAntiKt4TruthJets_ghostB_pt\"\n",
    "  , \"AnalysisAntiKt4TruthJets_ghostB_eta\"\n",
    "  , \"AnalysisAntiKt4TruthJets_ghostB_phi\"\n",
    "  ]\n",
    "\n",
    "# true b-hadron information\n",
    "# these b-hadrons are inside the truth jets\n",
    "bhadfeatures = \\\n",
    "   [ \"AnalysisAntiKt4TruthJets_ghostB_pt\"\n",
    "   , \"AnalysisAntiKt4TruthJets_ghostB_eta\"\n",
    "   , \"AnalysisAntiKt4TruthJets_ghostB_phi\"\n",
    "   , \"AnalysisAntiKt4TruthJets_ghostB_m\"\n",
    "   ]\n",
    "  \n",
    "\n",
    "# reconstructed track information\n",
    "trackfeatures = \\\n",
    "  [ \"AnalysisTracks_pt\"\n",
    "  , \"AnalysisTracks_eta\"\n",
    "  , \"AnalysisTracks_phi\"\n",
    "  #, \"AnalysisTracks_z0sinTheta\"\n",
    "  #, \"AnalysisTracks_d0sig\"\n",
    "  #, \"AnalysisTracks_d0\"\n",
    "  #, \"AnalysisTracks_d0sigPV\"\n",
    "  #, \"AnalysisTracks_d0PV\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the requested branches from the file\n",
    "features = tree.arrays(jetfeatures + trackfeatures + branches, entry_stop=MAXEVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find where angular distance is small\n",
    "def matchTracks(jets, trks):\n",
    "  jeteta = jets[\"AnalysisAntiKt4TruthJets_eta\"] \n",
    "  jetphi = jets[\"AnalysisAntiKt4TruthJets_phi\"]\n",
    "\n",
    "  trketas = trks[\"AnalysisTracks_eta\"]\n",
    "  trkphis = trks[\"AnalysisTracks_phi\"]\n",
    "\n",
    "  detas = jeteta - trketas\n",
    "  dphis = numpy.abs(jetphi - trkphis)\n",
    "\n",
    "  # deal with delta phis being annoying\n",
    "  awkward.where(dphis > numpy.pi, dphis - numpy.pi, dphis)\n",
    "\n",
    "  return numpy.sqrt(dphis**2 + detas**2) < 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting from polar to cartesian\n",
    "\n",
    "#Used for jets\n",
    "def ptetaphi2pxpypz(ptetaphi):\n",
    "  pts = ptetaphi[:,0:1]\n",
    "  etas = ptetaphi[:,1:2]\n",
    "  phis = ptetaphi[:,2:3]\n",
    "\n",
    "  pxs = pts * numpy.cos(phis)\n",
    "  pys = pts * numpy.sin(phis)\n",
    "  pzs = pts * numpy.sinh(etas)\n",
    "\n",
    "  isinf = numpy.isinf(pzs)\n",
    "\n",
    "  if numpy.any(isinf):\n",
    "    print(\"inf from eta:\")\n",
    "    print(etas[isinf])\n",
    "    raise ValueError(\"infinity from sinh(eta)\")\n",
    "\n",
    "  return numpy.concatenate([pxs, pys, pzs], axis=1)\n",
    "\n",
    "#Used for tracks\n",
    "def ptetaphi2pxpypz2(ptetaphi):\n",
    "  pts = ptetaphi[:,:,0:1]\n",
    "  etas = ptetaphi[:,:,1:2]\n",
    "  phis = ptetaphi[:,:,2:3]\n",
    "\n",
    "  mask = pts == MASKVAL\n",
    "  #Looking in array and testing a condition - if finds mask, replaces mask with pt value\n",
    "  pxs = numpy.where(mask, pts, pts * numpy.cos(phis)) # Apply transformation only to actual pT\n",
    "  pys = numpy.where(mask, pts, pts * numpy.sin(phis))\n",
    "  pzs = numpy.where(mask, pts, pts * numpy.sinh(etas))\n",
    "\n",
    "  isinf = numpy.isinf(pzs)\n",
    "\n",
    "  if numpy.any(isinf):\n",
    "    print(\"inf from eta:\")\n",
    "    print(etas[isinf])\n",
    "    raise ValueError(\"infinity from sinh(eta)\")\n",
    "\n",
    "  return numpy.concatenate([pxs, pys, pzs], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads inputs with nans up to the given maxsize\n",
    "def pad(xs, maxsize):\n",
    "  #Find 'none' values in array and replace with MASKVAL (= fill_none)\n",
    "  ys = \\\n",
    "    awkward.fill_none \\\n",
    "  ( awkward.pad_none(xs, maxsize, axis=1, clip=True) #Adding 'none' values to make sure it is correct size\n",
    "  , MASKVAL\n",
    "  )[:,:maxsize]\n",
    "\n",
    "  return awkward.to_regular(ys, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten1(xs, maxsize=-1):\n",
    "  ys = {}\n",
    "  for field in xs.fields:\n",
    "    zs = xs[field]\n",
    "    if maxsize > 0:\n",
    "      zs = pad(zs, maxsize)\n",
    "    ys[field] = zs\n",
    "\n",
    "  return awkward.zip(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define histogram plotting functions\n",
    "# returns a fixed set of bin edges\n",
    "def fixedbinning(xmin, xmax, nbins):\n",
    "  return numpy.mgrid[xmin:xmax:nbins*1j]\n",
    "\n",
    "\n",
    "# define two functions to aid in plotting\n",
    "def hist(xs, binning, normalized=False):\n",
    "  ys = numpy.histogram(xs, bins=binning)[0]\n",
    "\n",
    "  yerrs = numpy.sqrt(ys)\n",
    "\n",
    "  if normalized:\n",
    "    s = numpy.sum(ys)\n",
    "    ys = ys / s\n",
    "    yerrs = yerrs / s\n",
    "\n",
    "  return ys, yerrs\n",
    "\n",
    "\n",
    "def binneddensity(xs, binning, label=None, xlabel=None, ylabel=\"binned probability density\"):\n",
    "  fig = figure.Figure(figsize=(8, 8))\n",
    "  plt = fig.add_subplot(111)\n",
    "\n",
    "  ys , yerrs = hist(xs, binning, normalized=True)\n",
    "\n",
    "  # determine the central value of each histogram bin\n",
    "  # as well as the width of each bin\n",
    "  # this assumes a fixed bin size.\n",
    "  xs = (binning[1:]+binning[:-1]) / 2.0\n",
    "  xerrs = ((binning[1:]-binning[:-1]) / 2.0)\n",
    "\n",
    "  plt.errorbar \\\n",
    "    ( xs\n",
    "    , ys\n",
    "    , xerr=xerrs\n",
    "    , yerr=yerrs\n",
    "    , label=label\n",
    "    , linewidth=0\n",
    "    , elinewidth=2\n",
    "    )\n",
    "\n",
    "  plt.set_xlabel(xlabel)\n",
    "  plt.set_ylabel(ylabel)\n",
    "\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = \\\n",
    "  features[awkward.sum(features[\"AnalysisAntiKt4TruthJets_pt\"] > 25000, axis=1) > 0]\n",
    "\n",
    "jets1 = events[jetfeatures][:,0] #First jet\n",
    "tracks = events[trackfeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchedtracks = tracks[matchTracks(jets1, tracks)] \n",
    "matchedtracks = flatten1(matchedtracks, MAXTRACKS) #Turn into regular np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bjets = awkward.sum(jets1[\"AnalysisAntiKt4TruthJets_ghostB_pt\"] > 5000, axis=1) > 0 #Find b hadron jets with certain momentum\n",
    "jets2 = jets1[bjets] #Jets identified as b jets are only jets considered\n",
    "bhadspt= jets2[\"AnalysisAntiKt4TruthJets_ghostB_pt\"][:,0] #np Stack here - Each sub array contains all the features of the jet (axis -1)\n",
    "bhadseta = jets2[\"AnalysisAntiKt4TruthJets_ghostB_eta\"][:, 0]\n",
    "bhadsphi = jets2[\"AnalysisAntiKt4TruthJets_ghostB_phi\"][:,0]\n",
    "matchedtracks = matchedtracks[bjets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "jets3 = structured_to_unstructured(jets2[jetfeatures[:-3]]) #number of features\n",
    "matchedtracks = structured_to_unstructured(matchedtracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7165/2834944321.py:32: RuntimeWarning: overflow encountered in sinh\n",
      "  pzs = numpy.where(mask, pts, pts * numpy.sinh(etas))\n"
     ]
    }
   ],
   "source": [
    "jets4 = ptetaphi2pxpypz(jets3).to_numpy()\n",
    "tracks = ptetaphi2pxpypz2(matchedtracks.to_numpy())\n",
    "bhadspt = bhadspt.to_numpy()\n",
    "bhadseta = bhadseta.to_numpy()\n",
    "bhadsphi = bhadsphi.to_numpy()\n",
    "bhads = numpy.stack([bhadspt, bhadseta, bhadsphi], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training model\n",
    "\n",
    "tracklayers = [ 32 , 32 , 32 , 32 , 32 ]\n",
    "jetlayers = [ 64 , 64 , 64 , 64 , 64 ]\n",
    "\n",
    "def buildModel(tlayers, jlayers, ntargets):\n",
    "  inputs = layers.Input(shape=(None, tlayers[0]))\n",
    "\n",
    "  outputs = inputs\n",
    "  outputs = layers.Masking(mask_value=MASKVAL)(outputs)\n",
    "\n",
    "  for nodes in tlayers[:-1]:\n",
    "    outputs = layers.Dropout(0.3)(outputs)\n",
    "    outputs = layers.TimeDistributed(layers.Dense(nodes, activation='relu'))(outputs)\n",
    "    outputs = layers.BatchNormalization()(outputs)\n",
    "\n",
    "  outputs = layers.TimeDistributed(layers.Dense(tlayers[-1], activation='softmax'))(outputs)\n",
    "  outputs = Sum()(outputs)\n",
    "\n",
    "  for nodes in jlayers:\n",
    "    outputs = layers.Dropout(0.3)(outputs)\n",
    "    outputs = layers.Dense(nodes, activation='relu')(outputs)\n",
    "    outputs = layers.BatchNormalization()(outputs)\n",
    "\n",
    "  outputs = layers.Dense(ntargets + ntargets*(ntargets+1)//2)(outputs)\n",
    "\n",
    "  return \\\n",
    "    keras.Model \\\n",
    "    ( inputs = inputs\n",
    "    , outputs = outputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalise loss to n targets\n",
    "##Convert b hadron features to cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function\n",
    "# this ignores any dimension beyond the first!\n",
    "def LogNormal1D(true, meanscovs):\n",
    "  ntargets = true.shape[1] #Number of variables predicting\n",
    "  means = meanscovs[:,:ntargets] #First n targets are the means\n",
    "  # ensure diagonal is positive\n",
    "  logsigma = meanscovs[:,ntargets:2*ntargets]\n",
    "  rest = meanscovs[:,2*ntargets:]\n",
    "\n",
    "  # TODO\n",
    "  # build matrix\n",
    "  loss = 0\n",
    "  for x in range(ntargets):\n",
    "    loss = loss + ((means[:,x] - true[:,x])**2 / (2*keras.backend.exp(logsigma[:,x])**2)) + logsigma[:,x]\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 09:31:41.246972: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-21 09:31:41.247006: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-21 09:31:41.247024: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vonneumann.csc.warwick.ac.uk): /proc/driver/nvidia/version does not exist\n",
      "2022-11-21 09:31:41.247231: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = buildModel([len(trackfeatures)] + tracklayers, jetlayers, 3)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "model.compile \\\n",
    "  ( loss = LogNormal1D\n",
    "  , optimizer = keras.optimizers.Adam(learning_rate=LR)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68143, 8, 3)\n",
      "[1.37346188e+05 8.16028237e-01 1.20712149e+00]\n",
      "[[ 6209.26450298 13355.34468861 12169.39804895]\n",
      " [  332.31549738  3367.86154568  3007.97056318]\n",
      " [13929.08789537 37259.15905185 37689.85826002]\n",
      " [  895.76702185  1391.20435716  1802.68682528]\n",
      " [  126.80887282   734.25512833   518.3439794 ]\n",
      " [  137.88256407  2120.73090004  2343.21656601]\n",
      " [  210.391507    5898.674895    5550.99705111]\n",
      " [ -999.          -999.          -999.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Splits the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(tracks, bhads, train_size = 0.75)\n",
    "print(numpy.shape(tracks))\n",
    "print(bhads[0])\n",
    "print(tracks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "799/799 [==============================] - 6s 5ms/step - loss: 65041469440.0000 - val_loss: 12115169280.0000 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 7359624192.0000 - val_loss: 7868519424.0000 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 4623995392.0000 - val_loss: 4086514944.0000 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 3003046144.0000 - val_loss: 2258329600.0000 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 1933797248.0000 - val_loss: 1423981184.0000 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 1161457280.0000 - val_loss: 790967744.0000 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 647088768.0000 - val_loss: 471929824.0000 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 350596224.0000 - val_loss: 242919664.0000 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 179105968.0000 - val_loss: 113929648.0000 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 96118536.0000 - val_loss: 66113568.0000 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 51620464.0000 - val_loss: 35692988.0000 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 29385406.0000 - val_loss: 20861474.0000 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 16981728.0000 - val_loss: 12273230.0000 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 10104372.0000 - val_loss: 7145888.5000 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 6115254.5000 - val_loss: 4218689.0000 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 3720504.5000 - val_loss: 2805048.2500 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 2292200.2500 - val_loss: 1689302.3750 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 1386389.1250 - val_loss: 1047103.3750 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 867960.6250 - val_loss: 633188.6875 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 556937.1250 - val_loss: 406083.9688 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 337340.6250 - val_loss: 252452.4219 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 218375.1406 - val_loss: 164172.7344 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 135768.2344 - val_loss: 106131.1719 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 88422.5078 - val_loss: 66814.0469 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 55507.8008 - val_loss: 44469.5820 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 35780.5820 - val_loss: 28677.5039 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 23264.3320 - val_loss: 18501.8750 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 14971.0195 - val_loss: 11568.0791 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 9447.9180 - val_loss: 7891.4121 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 6206.2207 - val_loss: 5130.1045 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 3988.9202 - val_loss: 3465.7947 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 2618.1440 - val_loss: 2072.4353 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 1719.4969 - val_loss: 1446.7551 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 1115.2070 - val_loss: 942.8004 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 729.0692 - val_loss: 624.0173 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 473.5329 - val_loss: 376.1190 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 312.1383 - val_loss: 260.7303 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 207.2951 - val_loss: 174.8432 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 142.5151 - val_loss: 121.1027 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 95.5909 - val_loss: 80.2723 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 65.8695 - val_loss: 57.4404 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 47.7340 - val_loss: 40.0605 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 35.6338 - val_loss: 31.2199 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 27.4883 - val_loss: 24.8785 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 22.3099 - val_loss: 20.7022 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 19.2338 - val_loss: 17.6636 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 17.0031 - val_loss: 16.0822 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 15.6812 - val_loss: 14.9963 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 14.9145 - val_loss: 14.4065 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 14.4242 - val_loss: 14.0879 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 14.1747 - val_loss: 13.8715 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 14.0431 - val_loss: 13.7924 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 14.0109 - val_loss: 13.7556 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.9744 - val_loss: 13.7497 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 13.9784 - val_loss: 13.7299 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 13.9676 - val_loss: 13.7245 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.9738 - val_loss: 13.7435 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.9637 - val_loss: 13.7291 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.9582 - val_loss: 13.7232 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 13.9367 - val_loss: 13.7342 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 13.9373 - val_loss: 13.7438 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 13.9269 - val_loss: 13.7149 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 13.9056 - val_loss: 13.7270 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 13.8746 - val_loss: 13.7241 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.8480 - val_loss: 13.7006 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 13.8190 - val_loss: 13.6695 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.7860 - val_loss: 13.6379 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.7222 - val_loss: 13.5833 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.6471 - val_loss: 13.4551 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 13.5632 - val_loss: 13.3443 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.4647 - val_loss: 13.1982 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 13.3409 - val_loss: 12.9768 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 13.1588 - val_loss: 12.6190 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 12.9488 - val_loss: 12.5104 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.8283 - val_loss: 12.3617 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.7042 - val_loss: 12.3488 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.5721 - val_loss: 12.1800 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.4838 - val_loss: 12.0383 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.4364 - val_loss: 11.9536 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.3699 - val_loss: 11.9611 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.3169 - val_loss: 11.8759 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.2851 - val_loss: 11.8423 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 12.2484 - val_loss: 11.9344 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.2275 - val_loss: 12.0929 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.1920 - val_loss: 12.5762 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.1916 - val_loss: 11.7697 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.1512 - val_loss: 11.7021 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.1322 - val_loss: 13.8102 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.1148 - val_loss: 11.6559 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.1117 - val_loss: 14.0278 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0887 - val_loss: 11.5929 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0912 - val_loss: 13.8662 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 12.0680 - val_loss: 14.2751 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0471 - val_loss: 11.6357 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0505 - val_loss: 13.6085 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0540 - val_loss: 13.0262 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0466 - val_loss: 11.5855 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 12.0269 - val_loss: 11.5686 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0291 - val_loss: 12.0996 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0223 - val_loss: 14.0428 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0213 - val_loss: 11.4948 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 12.0114 - val_loss: 13.0162 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.9820 - val_loss: 11.5523 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9738 - val_loss: 11.5112 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9924 - val_loss: 14.5550 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.9894 - val_loss: 14.5284 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9621 - val_loss: 14.6823 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.9616 - val_loss: 11.5219 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9741 - val_loss: 12.3240 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9503 - val_loss: 11.4879 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.9584 - val_loss: 11.4412 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.9332 - val_loss: 18.5178 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9279 - val_loss: 14.0444 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9453 - val_loss: 11.4679 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9343 - val_loss: 12.4398 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.9345 - val_loss: 12.7452 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.9193 - val_loss: 11.3967 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9160 - val_loss: 11.5508 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.9086 - val_loss: 17.8378 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.9197 - val_loss: 12.9328 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9051 - val_loss: 11.5395 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.9051 - val_loss: 12.0799 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9155 - val_loss: 13.3787 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.9026 - val_loss: 14.9997 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8899 - val_loss: 11.4843 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8913 - val_loss: 13.2670 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.9172 - val_loss: 11.5443 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8827 - val_loss: 11.4343 - lr: 8.0000e-04\n",
      "Epoch 129/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8702 - val_loss: 11.5053 - lr: 8.0000e-04\n",
      "Epoch 130/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8797 - val_loss: 11.5170 - lr: 8.0000e-04\n",
      "Epoch 131/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8770 - val_loss: 11.4580 - lr: 8.0000e-04\n",
      "Epoch 132/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8789 - val_loss: 11.3790 - lr: 8.0000e-04\n",
      "Epoch 133/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8620 - val_loss: 14.0537 - lr: 8.0000e-04\n",
      "Epoch 134/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8776 - val_loss: 15.8720 - lr: 8.0000e-04\n",
      "Epoch 135/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8542 - val_loss: 15.7578 - lr: 8.0000e-04\n",
      "Epoch 136/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8576 - val_loss: 11.4497 - lr: 8.0000e-04\n",
      "Epoch 137/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8690 - val_loss: 13.6289 - lr: 8.0000e-04\n",
      "Epoch 138/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8575 - val_loss: 16.2155 - lr: 8.0000e-04\n",
      "Epoch 139/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8644 - val_loss: 19.1340 - lr: 8.0000e-04\n",
      "Epoch 140/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8460 - val_loss: 14.1013 - lr: 8.0000e-04\n",
      "Epoch 141/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8432 - val_loss: 11.4057 - lr: 8.0000e-04\n",
      "Epoch 142/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8479 - val_loss: 11.3602 - lr: 8.0000e-04\n",
      "Epoch 143/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8482 - val_loss: 14.8851 - lr: 8.0000e-04\n",
      "Epoch 144/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8411 - val_loss: 11.3901 - lr: 8.0000e-04\n",
      "Epoch 145/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8647 - val_loss: 16.5907 - lr: 8.0000e-04\n",
      "Epoch 146/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8719 - val_loss: 14.9500 - lr: 8.0000e-04\n",
      "Epoch 147/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8431 - val_loss: 15.3414 - lr: 8.0000e-04\n",
      "Epoch 148/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8446 - val_loss: 15.6093 - lr: 8.0000e-04\n",
      "Epoch 149/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8519 - val_loss: 11.3950 - lr: 8.0000e-04\n",
      "Epoch 150/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8704 - val_loss: 14.7843 - lr: 8.0000e-04\n",
      "Epoch 151/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8240 - val_loss: 11.6697 - lr: 8.0000e-04\n",
      "Epoch 152/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.8458 - val_loss: 16.1626 - lr: 8.0000e-04\n",
      "Epoch 153/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8398 - val_loss: 11.3991 - lr: 6.4000e-04\n",
      "Epoch 154/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8174 - val_loss: 14.3379 - lr: 6.4000e-04\n",
      "Epoch 155/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8161 - val_loss: 18.5639 - lr: 6.4000e-04\n",
      "Epoch 156/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8489 - val_loss: 17.6048 - lr: 6.4000e-04\n",
      "Epoch 157/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8279 - val_loss: 16.9536 - lr: 6.4000e-04\n",
      "Epoch 158/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8365 - val_loss: 11.3493 - lr: 6.4000e-04\n",
      "Epoch 159/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8131 - val_loss: 19.0179 - lr: 6.4000e-04\n",
      "Epoch 160/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8188 - val_loss: 15.8267 - lr: 6.4000e-04\n",
      "Epoch 161/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8140 - val_loss: 16.4977 - lr: 6.4000e-04\n",
      "Epoch 162/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8039 - val_loss: 15.6254 - lr: 6.4000e-04\n",
      "Epoch 163/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8242 - val_loss: 13.1779 - lr: 6.4000e-04\n",
      "Epoch 164/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8037 - val_loss: 11.3491 - lr: 6.4000e-04\n",
      "Epoch 165/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7990 - val_loss: 11.4036 - lr: 6.4000e-04\n",
      "Epoch 166/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8167 - val_loss: 11.4001 - lr: 6.4000e-04\n",
      "Epoch 167/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8085 - val_loss: 14.5187 - lr: 6.4000e-04\n",
      "Epoch 168/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8092 - val_loss: 16.2675 - lr: 6.4000e-04\n",
      "Epoch 169/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8058 - val_loss: 19.8699 - lr: 6.4000e-04\n",
      "Epoch 170/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8221 - val_loss: 18.1325 - lr: 6.4000e-04\n",
      "Epoch 171/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8068 - val_loss: 15.5212 - lr: 6.4000e-04\n",
      "Epoch 172/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8054 - val_loss: 17.3752 - lr: 6.4000e-04\n",
      "Epoch 173/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.8090 - val_loss: 16.7058 - lr: 6.4000e-04\n",
      "Epoch 174/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.8107 - val_loss: 16.5799 - lr: 6.4000e-04\n",
      "Epoch 175/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7985 - val_loss: 11.3298 - lr: 5.1200e-04\n",
      "Epoch 176/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7863 - val_loss: 15.9324 - lr: 5.1200e-04\n",
      "Epoch 177/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.7924 - val_loss: 16.6901 - lr: 5.1200e-04\n",
      "Epoch 178/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.7750 - val_loss: 16.0612 - lr: 5.1200e-04\n",
      "Epoch 179/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.7898 - val_loss: 17.0935 - lr: 5.1200e-04\n",
      "Epoch 180/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.7818 - val_loss: 15.6967 - lr: 5.1200e-04\n",
      "Epoch 181/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7773 - val_loss: 15.6980 - lr: 5.1200e-04\n",
      "Epoch 182/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.7959 - val_loss: 15.9917 - lr: 5.1200e-04\n",
      "Epoch 183/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.7916 - val_loss: 15.2627 - lr: 5.1200e-04\n",
      "Epoch 184/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.7962 - val_loss: 15.9855 - lr: 5.1200e-04\n",
      "Epoch 185/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.7748 - val_loss: 16.4825 - lr: 5.1200e-04\n",
      "Epoch 186/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7750 - val_loss: 14.4585 - lr: 4.0960e-04\n",
      "Epoch 187/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7864 - val_loss: 18.4383 - lr: 4.0960e-04\n",
      "Epoch 188/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7758 - val_loss: 18.7059 - lr: 4.0960e-04\n",
      "Epoch 189/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7865 - val_loss: 18.7085 - lr: 4.0960e-04\n",
      "Epoch 190/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7824 - val_loss: 11.3297 - lr: 4.0960e-04\n",
      "Epoch 191/1000\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 11.7786 - val_loss: 16.8131 - lr: 4.0960e-04\n",
      "Epoch 192/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.7695 - val_loss: 18.4207 - lr: 4.0960e-04\n",
      "Epoch 193/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.7581 - val_loss: 18.5096 - lr: 4.0960e-04\n",
      "Epoch 194/1000\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 11.7814 - val_loss: 18.1854 - lr: 4.0960e-04\n",
      "Epoch 195/1000\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 11.7793 - val_loss: 18.8822 - lr: 4.0960e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trains the data\n",
    "history = model.fit(X_train, y_train, validation_data = (X_valid, y_valid), batch_size=BATCHSIZE, callbacks =[early_stopping, reduce_lr], epochs=EPOCHS)\n",
    "\n",
    "#Plots and saves the loss curve\n",
    "history_df = numpy.log(pd.DataFrame(history.history))\n",
    "fig=plt.figure()\n",
    "LossFigure = history_df.loc[:, ['loss', 'val_loss']].plot().get_figure()\n",
    "LossFigure.savefig('Loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model/assets\n"
     ]
    }
   ],
   "source": [
    "#Saves the model\n",
    "model.save('Model')\n",
    "model.save_weights('Model Weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 [==============================] - 2s 1ms/step\n",
      "Pred 0 =  [ 1.5799552e+03  6.0997295e-01  1.1196030e+00  1.1447669e+01\n",
      " -1.1552682e+00 -1.3786500e+00 -2.3520100e+00 -5.0825751e-01\n",
      "  8.4628886e-01]\n",
      "Bhads 0 =  [1.37346188e+05 8.16028237e-01 1.20712149e+00]\n",
      "exp pred 0 =  [          inf 1.8403817e+00 3.0636377e+00 9.3682711e+04 3.1497306e-01\n",
      " 2.5191844e-01 9.5177658e-02 6.0154283e-01 2.3309801e+00]\n",
      "exp pred 4 0 =  0.31497306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7165/3073143090.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  print(\"exp pred 0 = \", numpy.exp(pred[0]))\n"
     ]
    }
   ],
   "source": [
    "# Uses the model to predict validation set\n",
    "pred = model.predict(tracks)\n",
    "print(\"Pred 0 = \", pred[0])\n",
    "print(\"Bhads 0 = \", bhads[0])\n",
    "print(\"exp pred 0 = \", numpy.exp(pred[0]))\n",
    "print(\"exp pred 4 0 = \", numpy.exp(pred[:,4][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.447669 11.395547 11.365636 ... 11.483423 11.358631 11.466135]\n",
      "[[ 1.37346188e+05  8.16028237e-01  1.20712149e+00]\n",
      " [ 7.71383594e+04  1.58197463e+00  1.55986178e+00]\n",
      " [ 9.88936328e+04 -2.40611649e+00 -2.47372627e+00]\n",
      " ...\n",
      " [ 9.80441562e+04 -4.61435974e-01  1.35823727e+00]\n",
      " [ 1.35274062e+05  2.03558505e-01  1.47211230e+00]\n",
      " [ 8.85507578e+04  2.74742275e-01  3.76518816e-02]]\n",
      "pT max, min, mean = -80500.21571101296 -80500.21571101296 -80500.21571101296\n",
      "Pull max, min, mean = -0.03205222332390968 -8.151986672710276 -0.8967839147916601\n",
      "eta max, min, mean = 4.332607209682465 -4.269299566745758 -0.013792201614586792\n",
      "Scaled eta max, min, mean = 5.66898145122972 -5.095958072709008 -0.0955206947877305\n",
      "Phi max, min, mean = 6.002540111541748 -5.423507213592529 0.047878055206113976\n",
      "Scaled Phi max, min, mean = 11.778710217077373 -6.425721486434648 0.05635452053803596\n"
     ]
    }
   ],
   "source": [
    "# Saves the variables we want from the model\n",
    "pTDiff=pred[:,0] - bhads[:,0]\n",
    "pTErr= numpy.exp(pred[:,3])\n",
    "Pull = pTDiff/pTErr\n",
    "\n",
    "\n",
    "etaDiff = pred[:,1] - bhads[:,1]\n",
    "etaErr = numpy.exp(pred[:, 4])\n",
    "scaledEtaDiff = etaDiff/etaErr\n",
    "print('eta max, min, mean = ' + str(numpy.max(etaDiff)), str(numpy.min(etaDiff)), str(numpy.mean(etaDiff)))\n",
    "print('Scaled eta max, min, mean = ' + str(numpy.max(scaledEtaDiff)), str(numpy.min(scaledEtaDiff)), str(numpy.mean(scaledEtaDiff)))\n",
    "\n",
    "phiDiff = pred[:,2] - bhads[:,2]\n",
    "phiErr = numpy.exp(pred[:, 5])\n",
    "scaledPhiDiff = phiDiff / phiErr\n",
    "print('Phi max, min, mean = ' + str(numpy.max(phiDiff)), str(numpy.min(phiDiff)), str(numpy.mean(phiDiff)))\n",
    "print('Scaled Phi max, min, mean = ' + str(numpy.max(scaledPhiDiff)), str(numpy.min(scaledPhiDiff)), str(numpy.mean(scaledPhiDiff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the plots\n",
    "#The loss curve\n",
    "\n",
    "\n",
    "# The Histograms\n",
    "fig = \\\n",
    "binneddensity \\\n",
    "( Pull\n",
    ", fixedbinning(-5, 5, 100)\n",
    ", xlabel=\"Pull\"\n",
    ")\n",
    "fig.savefig(\"Pull.png\")\n",
    "\n",
    "fig = \\\n",
    "binneddensity \\\n",
    "( pTDiff\n",
    ", fixedbinning(-200000, 200000, 100)\n",
    ", xlabel=\"pT Difference\"\n",
    ")\n",
    "fig.savefig(\"pT Difference.png\")\n",
    "\n",
    "fig = \\\n",
    "binneddensity \\\n",
    "( etaDiff\n",
    ", fixedbinning(-5, 5, 100)\n",
    ", xlabel=\"Eta Diff\"\n",
    ")\n",
    "fig.savefig(\"Eta Diff.png\")\n",
    "\n",
    "fig = \\\n",
    "binneddensity \\\n",
    "( scaledEtaDiff\n",
    ", fixedbinning(-5, 5, 100)\n",
    ", xlabel=\"Scaled Eta Diff\"\n",
    ")\n",
    "fig.savefig(\"scaled Eta Diff.png\")\n",
    "\n",
    "fig = \\\n",
    "binneddensity \\\n",
    "( phiDiff\n",
    ", fixedbinning(-5, 5, 100)\n",
    ", xlabel=\"Phi Diff\"\n",
    ")\n",
    "fig.savefig(\"Phi Diff.png\")\n",
    "\n",
    "fig = \\\n",
    "binneddensity \\\n",
    "( scaledPhiDiff\n",
    ", fixedbinning(-5, 5, 100)\n",
    ", xlabel=\"Scaled Phi Diff\"\n",
    ")\n",
    "fig.savefig(\"Scaled Phi Diff.png\")\n",
    "\n",
    "#Input plots\n",
    "jetz0sintheta = features[\"AnalysisTracks_z0sinTheta\"]\n",
    "impactParam = features[\"AnalysisTracks_d0\"]\n",
    "impactParamSig = features[\"AnalysisTracks_d0sig\"]\n",
    "impactParamPV = features[\"AnalysisTracks_d0PV\"]\n",
    "impactParamPVSig = features[\"AnalysisTracks_d0sigPV\"]\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( jetz0sintheta[:,0] # the first jet in each event\n",
    "  , fixedbinning(-1, 1, 500) # 50 bins from 0 to 2\n",
    "  , xlabel=\"Transverse IP * sin(Theta)\"\n",
    "  )\n",
    "fig.savefig(\"TransverseIP.png\")\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( impactParam[:,0] # the first jet in each event\n",
    "  , fixedbinning(-1, 1, 500) # 50 bins from 0 to 2\n",
    "  , xlabel=\"Impact Parameter\"\n",
    "  )\n",
    "fig.savefig(\"Impact Parameter.png\")\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( impactParamSig[:,0] # the first jet in each event\n",
    "  , fixedbinning(-4, 4, 100) # 50 bins from 0 to 2\n",
    "  , xlabel=\"Impact Parameter Sig\"\n",
    "  )\n",
    "fig.savefig(\"Impact Parameter Sig.png\")\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( impactParamPV[:,0] # the first jet in each event\n",
    "  , fixedbinning(-0.5, 0.5, 250) # 50 bins from 0 to 2\n",
    "  , xlabel=\"Impact Parameter (PV)\"\n",
    "  )\n",
    "fig.savefig(\"Impact Parameter PV.png\")\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( impactParamPVSig[:,0] # the first jet in each event\n",
    "  , fixedbinning(-4, 4, 100) # 100 bins from -3 to 3\n",
    "  , xlabel=\"Impact Parameter Sig (PV)\"\n",
    "  )\n",
    "fig.savefig(\"Impact Parameter PV Sig.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pT Diff Mean = -80500.21571101296\n",
      "pT Diff Median = -72636.23315429688\n",
      "pT Diff Std = 42394.12964007519\n",
      "pT Diff IQR = 46339.808532714844\n",
      "Pull Mean = -0.8967839147916601\n",
      "Pull Median = -0.8141169630021852\n",
      "Pull Std = 0.4621993441448002\n",
      "Pull IQR = 0.5094972920245513\n"
     ]
    }
   ],
   "source": [
    "#Print any results we want\n",
    "print('pT Diff Mean = ' + str(numpy.mean(pTDiff)))\n",
    "print('pT Diff Median = ' + str(numpy.median(pTDiff)))\n",
    "print('pT Diff Std = ' + str(numpy.std(pTDiff)))\n",
    "print('pT Diff IQR = ' + str(numpy.percentile(pTDiff, 75)-numpy.percentile(pTDiff, 25)))\n",
    "print('Pull Mean = ' + str(numpy.mean(Pull)))\n",
    "print('Pull Median = ' + str(numpy.median(Pull)))\n",
    "print('Pull Std = ' + str(numpy.std(Pull)))\n",
    "print('Pull IQR = ' + str(numpy.percentile(Pull, 75)-numpy.percentile(Pull, 25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PDG ID: 531\n",
      "number of b-hadrons: 13487\n",
      "\n",
      "PDG ID: -521\n",
      "number of b-hadrons: 61531\n",
      "\n",
      "PDG ID: -511\n",
      "number of b-hadrons: 61243\n",
      "\n",
      "PDG ID: 5232\n",
      "number of b-hadrons: 751\n",
      "\n",
      "PDG ID: 511\n",
      "number of b-hadrons: 61706\n",
      "\n",
      "PDG ID: 521\n",
      "number of b-hadrons: 61594\n",
      "\n",
      "PDG ID: -531\n",
      "number of b-hadrons: 13526\n",
      "\n",
      "PDG ID: -5122\n",
      "number of b-hadrons: 5447\n",
      "\n",
      "PDG ID: 5122\n",
      "number of b-hadrons: 5232\n",
      "\n",
      "PDG ID: -5132\n",
      "number of b-hadrons: 676\n",
      "\n",
      "PDG ID: 5132\n",
      "number of b-hadrons: 722\n",
      "\n",
      "PDG ID: -5232\n",
      "number of b-hadrons: 733\n",
      "\n",
      "PDG ID: 555\n",
      "number of b-hadrons: 3\n",
      "\n",
      "PDG ID: 553\n",
      "number of b-hadrons: 6\n",
      "\n",
      "PDG ID: -5332\n",
      "number of b-hadrons: 21\n",
      "\n",
      "PDG ID: 5332\n",
      "number of b-hadrons: 18\n",
      "\n",
      "PDG ID: 100553\n",
      "number of b-hadrons: 1\n",
      "\n",
      "PDG ID: -541\n",
      "number of b-hadrons: 5\n",
      "\n",
      "PDG ID: 10551\n",
      "number of b-hadrons: 3\n",
      "\n",
      "PDG ID: 541\n",
      "number of b-hadrons: 5\n",
      "\n",
      "PDG ID: 20553\n",
      "number of b-hadrons: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quit()\n",
    "# this creates histograms of truth jet values.\n",
    "truthjetspt = features[\"AnalysisAntiKt4TruthJets_pt\"] \n",
    "truthjetseta = features[\"AnalysisAntiKt4TruthJets_eta\"]\n",
    "truthjetsphi = features[\"AnalysisAntiKt4TruthJets_phi\"]\n",
    "\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( truthjetspt[:,0] # the first jet in each event\n",
    "  , fixedbinning(0, 200000, 50) # 50 bins from 0 to 200000 MeV\n",
    "  , xlabel=\"First truth jet $p_T$ [MeV]\"\n",
    "  )\n",
    "\n",
    "fig.savefig(\"truthjet-pt-comb.png\")\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( truthjetseta[:,0] # the first jet in each event\n",
    "  , fixedbinning(-5, 5, 50) # 50 bins from 0 to 5\n",
    "  , xlabel=\"First truth jet pseudorepidity\"\n",
    "  )\n",
    "fig.savefig(\"truthjet-eta-comb.png\")\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( truthjetsphi[:,0] # the first jet in each event\n",
    "  , fixedbinning(0, 3.15, 50) # 50 bins from 0 to pi rad\n",
    "  , xlabel=\"First truth jet phi [rad]\"\n",
    "  )\n",
    "fig.savefig(\"truthjet-phi-comb.png\")\n",
    "\n",
    "\n",
    "# this creates histograms of the reconstructed jet values.\n",
    "reconstructedjetspt = features[\"AnalysisJets_pt_NOSYS\"]\n",
    "reconstructedjetseta = features[\"AnalysisJets_eta\"]\n",
    "reconstructedjetsphi = features[\"AnalysisJets_phi\"]\n",
    "\n",
    "\n",
    "pt_filt = []\n",
    "eta_filt = []\n",
    "phi_filt = []\n",
    "\n",
    "for x in range(len(reconstructedjetspt)):\n",
    "    if numpy.sum(numpy.absolute(reconstructedjetspt[x])) != 0:\n",
    "        pt_filt.append(reconstructedjetspt[x][0])\n",
    "    if numpy.sum(numpy.absolute(reconstructedjetseta[x])) != 0:\n",
    "        eta_filt.append(reconstructedjetseta[x][0])\n",
    "    if numpy.sum(numpy.absolute(reconstructedjetsphi[x])) != 0:\n",
    "        phi_filt.append(reconstructedjetsphi[x][0])\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( pt_filt # the first jet in each event\n",
    "  , fixedbinning(0, 200000, 50) # 50 bins from 0 to 200000 MeV\n",
    "  , xlabel=\"Reconstructed jet $p_T$ [MeV]\"\n",
    "  )\n",
    "fig.savefig(\"Reconstructedjet-pT-comb.png\")\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( eta_filt # the first jet in each event\n",
    "  , fixedbinning(-5, 5, 50) # 50 bins from -5 to 5\n",
    "  , xlabel=\"Reconstructed jet pseudorapidity\"\n",
    "  )\n",
    "fig.savefig(\"reconstructedjet-eta-comb.png\")\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( phi_filt # the first jet in each event\n",
    "  , fixedbinning(0, 3.15, 50) # 50 bins from 0 to pi rad\n",
    "  , xlabel=\"Reconstructed jet phi [rad]\"\n",
    "  )\n",
    "fig.savefig(\"reconstructedjet-phi-comb.png\")\n",
    "\n",
    "\n",
    "#this creates a histogram of b-hadron pTs\n",
    "b_hadron_pt = features[\"AnalysisAntiKt4TruthJets_ghostB_pt\"] \n",
    "b_hadron_pt = awkward.flatten(b_hadron_pt, axis = None)\n",
    "\n",
    "fig = \\\n",
    "  binneddensity \\\n",
    "  ( b_hadron_pt #the hadron pTs\n",
    "  , fixedbinning(0, 200000, 50) # 50 bins from 0 to 200000 MeV\n",
    "  , xlabel = \"B-Hadron $p_T$ [MeV]\"\n",
    "  )\n",
    "fig.savefig(\"ptDist.png\")\n",
    "\n",
    "# this counts the number of b-hadron identification numbers and prints out the\n",
    "# counts.\n",
    "# these are defined in the PDG (but no need to worry about the details for now!)\n",
    "# https://pdg.lbl.gov/2007/reviews/montecarlorpp.pdf\n",
    "\n",
    "\n",
    "truthbhadronsid = \\\n",
    "  awkward.flatten \\\n",
    "  ( features[\"AnalysisAntiKt4TruthJets_ghostB_pdgId\"]\n",
    "  , axis=None\n",
    "  )\n",
    "\n",
    "# loop over all the b-hadrons associated to jets\n",
    "counts = {}\n",
    "for bhadid in truthbhadronsid:\n",
    "  if bhadid in counts:\n",
    "    counts[bhadid] += 1\n",
    "  else:\n",
    "    counts[bhadid] = 1\n",
    "\n",
    "print()\n",
    "for bhadid in counts:\n",
    "  print(\"PDG ID:\", bhadid)\n",
    "  print(\"number of b-hadrons:\", counts[bhadid])\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Luke')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6679b2c4fdeb25d2a0cd890f27b2b235b752c32ed605315ad831eb2b3957fc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
